\documentclass{article}

% \usepackage[margin=0.75in]{geometry}

\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{epstopdf} 
\usepackage{caption}
\usepackage{amsmath}

\usepackage{stackengine}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}

\titleformat{\section}
  {\normalfont\Large\bfseries}   % The style of the section title
  {}                             % a prefix
  {0pt}                          % How much space exists between the prefix and the title
  {Question \thesection:\quad}    % How the section is represented

% Starred variant
\titleformat{name=\section,numberless}
  {\normalfont\Large\bfseries}
  {}
  {0pt}
  {}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\begin{document}
\title{Comp 6321 - Machine Learning - Assignment 1}
\author{Federico O'Reilly Regueiro}
\date{September 30th, 2016}
\maketitle

%------------------------ Q1 ------------------------%
\section{} 
%------------------------  a ------------------------%
\subsection{Load and plot data}
Code from A1\textunderscore driver.m:
	\begin{verbatim}
	x = load('hw1x.dat');
	y = load('hw1y.dat');

	% plot x against y, use circles and avoid plotting connecting lines
	figh1 = figure(1);
	plot(x, y, 'o');
	title('data plot and regressions d = [1:3]');
	ylabel('y / hw(x)');
	xlabel('x');
	\end{verbatim} 
   

    See figure 1 for plots.

%------------------------  b ------------------------%
\subsection{Add vector of 1s and plot linear regression with the data}
Code from A1\textunderscore driver.m:
	\begin{verbatim}
	x = [x, ones(length(x),1)];

	% equivalent to w = inv(x'*x) * x'*y;
	w = (x' * x) \ x'*y;

	% just plot the lie between the endpoints
	endpoints = [min(x), max(x)];

	plot(endpoints, (w(1).* endpoints + w(2)), 'r');
	\end{verbatim}
%------------------------ c ------------------------%
\subsection{Function that will evaluate training error}
Code from trainingErr.m:
	\begin{verbatim}
	function jh = trainingErr(x, w, y)
  	hx = x*w;
	   % use mse, doesn't really matter but it's simpler to think of than total se
	   jh = sum((hx - y).^2)/(2*size(x,1));
	end
	\end{verbatim} 
The mean squared training error for the linear regression is 0.33336.
%------------------------ d ------------------------%
\subsection{Write PolyRegress(x,y,d)}
Code from polyRegress.m:

	\begin{verbatim}
	function [w, x_prime] = PolyRegress(x, y, d, normalize)
 		 if nargin < 4
  			  normalize = false;
 		 end
 		 if(d >= 1)
   			 x_prime = format_poly(x, d, normalize);
  		else
   			 error('Error, d must be larger or equal to 1');
 		end
  		w = (x_prime'*x_prime) \ x_prime'*y;
	end
    \end{verbatim}
And from format\textunderscore poly.m:

% helper for PolyRegress
	\begin{verbatim}
	function x_prime = format_poly(x,d) 
	    x_prime = repmat(x(:,1), 1, d+1).^(repmat((d:-1:0), size(x,1), 1));
	end
	\end{verbatim}
    
%------------------------ e ------------------------%
\subsection{Quadratic fit}
The mean squared training error for the quadratic fit is 0.12613. Refer to figure 1 for a plot of the quadratic fit.

The training error is significantly smaller and the hypothesis does not seem to be overfitting as it is still quite a broad approximation. Essentially, yes this seems like a better fit.
%------------------------ f ------------------------%
\subsection{Cubic fit}
The mean squared training error for the cubic fit is 0.10878. Refer to figure.

The training error is yet somewhat smaller and the hypothesis still does not seem to be overfitting. Essentially, yes this seems like a better fit.

\noindent%
\begin{minipage}{\linewidth}% to keep image and caption on one page
\makebox[\linewidth]{%        to center the image
	\includegraphics[width=4in, trim=1in 3in 1in 3in]{fig1}}
\captionof{figure}{Answers 1a to 1f}\label{fig1}%      only if needed  
\end{minipage}

%------------------------ g ------------------------%
\subsection{Suppose the data were stored in increasing value of y}
If the folds were taken from an ordered set of samples, each training iteration would be missing out on a complete section of the data as opposed to removing data which is somewhat equally distributed at each iteration. This would generate a hypothesis space with high variance during the cross validation.
Given that the bias-variance decomposition, as per the lecture slides, can be seen as:
\begin{equation}
EP(y − h(x))^{2} |x| = EP [(h(x) −\hat{ h}(x))2|x| + (f(x) −\hat{ h}(x))2 + E[(y − f(x))2|x|
\end{equation}
Where the first term, $ EP [(h(x) −\hat{ h}(x))2|x|$, is the hypothesis variance contribution to error and tends to be larger for higher-order polynomial fittings, we could expect to have k-fold cross validation choose lower orders as ideal.
We have performed 5-fold cross validation on the homework data in this way and the chosen order is lower than cross validation performed on non ordered data, see figures 2 through 4. The testing errors for this process were:
\begin{center}
\begin{tabular}{ c c }
	\textbf{d} & \textbf{testing error}\\
	\hline
	1 & 0.87981\\
	2 & 0.29838\\
	3 & 2.46895\\
\end{tabular}
\end{center}

%%% Plots for 1g %%%
\noindent%
\begin{minipage}{\linewidth}% to keep image and caption on one page
\makebox[\linewidth]{%        to center the image
	\includegraphics[width=4in, trim=1in 3in 1in 3in]{fig5}}
\captionof{figure}{Hypothesis space for 5-fold CV, d=1, with folds chosen from ordered data}\label{fig5}%
\end{minipage}

.\\

\noindent%
\begin{minipage}{\linewidth}% to keep image and caption on one page   
\makebox[\linewidth]{%        to center the image
	\includegraphics[width=4in, trim=1in 3in 1in 3in]{fig6}}
\captionof{figure}{Hypothesis space for 5-fold CV, d=2, with folds chosen from ordered data}\label{fig6}%
\end{minipage}

\noindent%
\begin{minipage}[t]{\linewidth}% to keep image and caption on one page
\makebox[\linewidth]{%        to center the image
	\includegraphics[width=4in, trim=1in 3in 1in 3in]{fig7}}
\captionof{figure}{Hypothesis space for 5-fold CV, d=3, with folds chosen from ordered data}\label{fig7}%
\end{minipage}
%%% End Plots %%%

%------------------------ h ------------------------%
\subsection{Implement five-fold cross-validation and find d}
\begin{verbatim} 
function [d, train_error, test_error] = k_fold_cv (x, y, k)
    % sure, we could solve for the general case, but...
    if mod(size(x,1), k)
        error('Matrix x must contain a multiple of k entries');
    end
    
    m = size(x,1);

        % ideally do this to make partitions randomly: 
        % idx_shuffle = randperm(m);
        % however, we keep the same order as given
        idx_shuffle  = 1:m;
    end
    
    % matlab abonimation for a do-while loop
    test_err_decreasing = true;

    % first two entries in the errors are the maximum possible fp num
    % just for the logic in the while loop (no do-while in matlab)
    % get rid of these two entries at the end
    test_error = repmat(realmax, 2, k);
    train_error = repmat(realmax, 2, k);

    d = 0;

    while (test_err_decreasing)

        d = d+1; 

        m_k = m/k;

        for i = 1:k
           idx_btm = idx_shuffle(1:(m_k*(i-1)));
           idx_tst = idx_shuffle(m_k*(i-1)+1:(m_k*i));
           idx_top = idx_shuffle((m_k*i)+1:m);
           x_train = [x(idx_btm,:);x(idx_top,:)];
           y_train = [y(idx_btm,:);y(idx_top,:)];
           x_test = x(idx_tst, :);
           y_test = y(idx_tst, :);

           % training
           [w, x_train_poly] = PolyRegress(x_train,y_train,d);
           jh_train = trainingErr(x_train_poly, w, y_train);           
           train_error(d+2, i) = jh_train;

           % validation
           x_test_poly = format_poly(x_test, d);
           jh_test = trainingErr(x_test_poly, w, y_test);
           test_error(d+2, i) = jh_test;
       
       % allow for one increment in the errors since  
       test_err_decreasing = ...
              (mean(test_error(d+2,:)) <= mean(test_error(d+1,:))) ...
           || (mean(test_error(d+2,:)) < mean(test_error(d,:)));       
    end

    % remove first two entries for errors vectors
    test_error(1:2,:) = [];
    train_error(1:2,:) = [];

    % d already stopped decreasing, optimal d has just been passed
    % since we're checking the last two errors, we'd have to check 
    % conditions if optimal d is d - 1 or d - 2, but in this case
    % it's simpler to just take the minimum as it will be equivalent  
    [dummy, d] = min(mean(test_error,2));
end
\end{verbatim}

The measured average test errors were:
\begin{center}
\begin{tabular}{ c c }
	\textbf{d} & \textbf{testing error}\\
	\hline
	1 & 0.346206\\
	2 & 0.132642\\
	3 & 0.123586\\
	4 & 0.016670\\
	5 & 0.016886\\
	6 & 0.013115\\
	7 & 0.013709\\
	8 & 0.113617\\
\end{tabular}
\end{center}

In figure 5, we can see the mean testing errors as a function of d as well as the finite difference of the errors, negative fd's are green while red fd's are red. We can observe that for d=5 there is a very small increase in the test errors yet the testing error at d=6 is lower than the testing error for order 4. We have chosen to disregard this small increase, possibly derived from numerical precission issues; a sort of low-pass filtering of the testing errors. Both Matlab and Octave issue a warning regarding an ill-conditioned matrix, so there are possibly rounding errors. This is explained by the magnitude of the higher order terms, which cause problems during matrix inversion.\\

.\\

    \noindent%
	\begin{minipage}[t]{\linewidth}% to keep image and caption on one page
		\makebox[\linewidth]{%        to center the image
		\includegraphics[width=4.8in, trim=1in 3in 1in 3in]{fig2}}
		\captionof{figure}{Testing errors and optimal degree of polynomial fit}\label{fig2}%
	\end{minipage}

%------------------------ i ------------------------%
\subsection{perform five-fold cross-validation on normalized columns}
The matlab code for k\textunderscore fold\textunderscore cv was modified to include a normalize parameter defaulting to false.
\begin{verbatim}

function [d, train_error, test_error] = k_fold_cv (x, y, k, normalize)
    ...    
    if nargin < 4
        normalize = false;
    end
    ...
\end{verbatim}

And in format\textunderscore poly.m:
\begin{verbatim}
  ...  
  if normalize
     x_prime = x_prime / (diag(max(x_prime)));
  end
\end{verbatim}

The measured average test errors for the normalized case were:
\begin{center}
\begin{tabular}{ c c }
	\textbf{d} & \textbf{testing error}\\
	\hline
	1 & 0.350810\\
	2 & 0.131584\\
	3 & 0.125125\\
	4 & 0.021239\\
	5 & 0.021349\\
	6 & 0.017636\\
	7 & 0.018425\\
	8 & 0.018583\\
\end{tabular}
\end{center}

Figure 6, compared to the left-hand-side of figure 5, shows an equal evolution and the mean testing errors are very close, possibly differing from the rounding errors noted above. The selected order is the same.\\

.\\

    \noindent%
	\begin{minipage}[t]{\linewidth}% to keep image and caption on one page
		\makebox[\linewidth]{%        to center the image
		\includegraphics[width=4.8in, trim=1in 3in 1in 3in]{fig3}}
		\captionof{figure}{Testing errors for normalized 5-fold CV}\label{fig3}%
	\end{minipage}

There is however an interesting trend, regarding the system's stability. Comparing figure 7 to figure 8, we note how the normalized system is much more stable and might result a better strategy when looking for higher order fittings.\\

 \noindent%
	\begin{minipage}[t]{\linewidth}% to keep image and caption on one page
		\makebox[\linewidth]{%        to center the image
		\includegraphics[width=4.8in, trim=1in 3in 1in 3in]{fig2higher}}
		\captionof{figure}{Evolution of testing errors in non-normalized CV}\label{fig2higher}%
	\end{minipage}

 \noindent%
	\begin{minipage}[t]{\linewidth}% to keep image and caption on one page
		\makebox[\linewidth]{%        to center the image
		\includegraphics[width=4.8in, trim=1in 3in 1in 3in]{fig3higher}}
		\captionof{figure}{Evolution of testing errors in normalized CV}\label{fig3higher}%
	\end{minipage}


%------------------------ j ------------------------%
\subsection{Show that normalization only affects scaling but has no other effects}
Let us take $s$ to be the reciprocal of the maximum value of the input vector $x$. Then we define the matrix $S$ to be:
\begin{equation}
S \delequal \begin{bmatrix}
    s^0 & 0 & 0 & \dots  & 0 \\
    0 & s^1 & 0 & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & \dots  & s^n
\end{bmatrix}
\end{equation}

We not that by definition $S = S^{T}$ Then, normalizing $X$ is equivalent to performing the multiplication $XS$.\\

This being the case, if we take $w_{N}$ to be the weigths obtained from linear regression from normalized data $XS$ setting $\nabla_{w_N}J({w_{N}})$ to zero is as follows:

\begin{equation}
\begin{split}
\nabla_{w_{N}}J({w_{N}})& = \nabla_{w_{N}}(XSw_N-Y)^{T}(XSw-Y)\\
		      & = \nabla_{w_{N}}(w_{N}^{T}S^{T}X^{T}XSw_{N}-Y^{T}XSw_{N} - w_{N}^{T}S^{T}X^{T}Y + Y^{T}Y)\\
		      & = 2S^{T}X^{T}XSw_{N} - 2S^{T}X^{T}Y
\end{split}
\end{equation}
From which follows, if we want to set $\nabla_{w_{N}}J = 0$, then:
\begin{equation}
\begin{split}
{w_{N}}& = (S^{T}X^{T}XS)^{-1}) (S^{T}X^{T}Y)\\
	   & = S^{-1}(X^{T}X)^{-1}X^{T}Y\\
	   & = S^{-1}w
\end{split}
\end{equation}

Thus the prediction:
\begin{equation}
\begin{split}
XS{w_{N}}& = XSS^{-1}w\\
	       & = Xw\\
\end{split}
\end{equation}

The coefficients $w_{N}$ will thus render an approximation that is squashed around $x = 1$, as can be seen in figure 9.\\

.\\

 \noindent%
	\begin{minipage}[t]{\linewidth}% to keep image and caption on one page
		\makebox[\linewidth]{%        to center the image
		\includegraphics[width=4.8in, trim=1in 3in 1in 3in]{fig4}}
		\captionof{figure}{Scaled output given by the normalization of inputs}\label{fig4}%
	\end{minipage}

%------------------------ Q2 ------------------------%
\section{}
%------------------------ a ------------------------%
\subsection{Show function expressed as matrix form}
\begin{equation}
\begin{split}
J(w) & = \sum_{i = 1}^{m} u_i \sum_{j = 1}^{n} (w_j x_{ij} - y_i)^2\\
        & = \sum_{i = 1}^{m} \sum_{j = 1}^{n} u_i (w_j x_{ij} - y_i)^2\\
        & = \sum_{i = 1}^{m}  u_i (w^{T} x_{i} - y_i)^2\\
        & = \sum_{i = 1}^{m}  u_i (\hat{y}_i - y_i)^2\\
        & = \sum_{i = 1}^{m}  (\hat{y}_i - y_i)^{T}  u_i (\hat{y}_i - y_i)\\
        & = (Xw-y)^{T}U(Xw-y)
\end{split}
\end{equation}
Where $U \delequal diag(u)$ is a diagonal matrix obtained from the weighting vector $u$.
%------------------------ b ------------------------%
\subsection{Compute the gradient of J(w) with respect to w} 
\begin{equation}
\begin{split}
	\nabla_wJ(w) & = \nabla_w (UXw-Uy)^{T} (Xw-y)\\
	                      & = \nabla_w w^{T}X^{T}U^{T}Xw - Y^{T}UXw - w^{T}X^{T}UY + Y^{T}U^{T}Y\\
	                      & = \nabla_w w^{T}X^{T}UXw - 2 Y^{T}UXw + Y^{T}U^{T}Y\\
			 & = 2X^{T}UXw - 2X^{T}U^{T}Y = 2X^{T}UXw - 2X^{T}UY
\end{split}
\end{equation}

Thus by setting $\nabla_wJ(w) = 0$ we obtain:

\begin{equation}
{w} = (X^{T}UX)^{-1}X^{T}UY\\
\end{equation}

From which follows that if all coefficients of the vector $u$ are 1, then weighted linear regression is equivalent to linear regression since $U$ becomes the identity matrix.
%------------------------ c ------------------------%
\subsection{Implement weighted linear regression}
From weighted\textunderscore linear\textunderscore regression.m:
\begin{verbatim}
function w = weighted_linear_regression(x, y, u)
    u = diag(u);
    w = (x' * u * x) \ x' * u * y;
end
\end{verbatim}
By varying the weights, we factor more or less of the error stimming from certain points in the cost function. Thus lower weight for a given point will cause linear regression to ignore it somewhat whereas higher weights will cause linear regression to strive to pass thwough the point. This can be observed in figure 10, where three regressions with different weight for the largest input value have been performed. Notice how the fit passes through the concerned point with a heavier weight.

.\\

 \noindent%
	\begin{minipage}[t]{\linewidth}% to keep image and caption on one page
		\makebox[\linewidth]{%        to center the image
		\includegraphics[width=4.8in, trim=1in 3in 1in 3in]{fig8}}
		\captionof{figure}{Varying weights for the maximum input value}\label{fig8}%
	\end{minipage}

%------------------------ d ------------------------%
\subsection{Draw an example of a data-set for which weighted lr would work better than unweighted} 

.\\

 \noindent%
	\begin{minipage}[t]{\linewidth}% to keep image and caption on one page
		\makebox[\linewidth]{%        to center the image
		\includegraphics[width=4.8in, trim=1in 3in 1in 3in]{fig9}}
		\captionof{figure}{Dataset where a series of values could look like a data capture problem}\label{fig9}%
	\end{minipage}

.\\

In this example, supposing we have knowledge from the domain indicating that the points at the bottom represent a data acquisition problem and say that our confidence in them is low. In such a case, we could use weighted linear regression as an a function of our confidence level.

%------------------------  Q3 ------------------------%
\section{Show that if the error is exponential with non-negative support, maximizing the likelihood does not lead to minimizing MSE. What is the criterion?} 

Analogously to what we saw in class, we would need to maximize the likelihood:

\begin{equation}
	maxL(w) = \prod_{i=i}^{n} \lambda e ^{-\lambda (y_i-h_w(x_i))}  I_{\rm I\!R_{\ge 0}}(y_i-h_w(x_i))\\
\end{equation}

This is analytically intractable but it is still easier think that in order to maximize this we must minimize $y_i - h_w(x_i)$ from the right side, as negative values will yield 0 likelihood.
Seen from this perspective, the criterion for fit would be a pessimistic predictor, trying to always predict less or equal to the lowest output for any given input. 


\end{document} 